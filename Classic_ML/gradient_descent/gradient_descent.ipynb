{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66552653",
   "metadata": {},
   "source": [
    "# Main Concepts\n",
    "\n",
    "- Gradient Descent: find minimum of cost function (error)\n",
    "- Steps:\n",
    "    - parameter initialization\n",
    "    - find gradients: taking partial derivatives with respect to params or including bias term in feature vector\n",
    "    - update values of weights and bias\n",
    "    - repeat until iteration meet or cost function converges\n",
    "- Main components:\n",
    "    - cost function: regression (MSE, RMSE, MAE)\n",
    "    - number of epochs: number of times to pass the entire training dataset\n",
    "    - learning rate\n",
    "- Note\n",
    "    - subjective to feature scale: large scale features dominate the change\n",
    "\n",
    "[source](https://prasad07143.medium.com/variants-of-gradient-descent-and-their-implementation-in-python-from-scratch-2b3cceb7a1a0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0279cc",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da28c7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2) (1000,)\n",
      "(800, 2) (200, 2) (800,) (200,)\n",
      "5.551115123125783e-18 1.0\n",
      "1.7763568394002505e-17 1.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model, model_selection, preprocessing, metrics, datasets\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Generating random regression data\n",
    "X, y = datasets.make_regression(n_samples = 1000, n_features = 2, noise = 7, random_state = 16)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2, random_state = 3)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# Feature Scaling ( Standardization or Z-Score Normalization )\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print(X_train.mean(), X_train.std())\n",
    "print(X_test.mean(), X_test.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4cc9b6",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent\n",
    "- Uses the whole batch of training instances to compute gradient\n",
    "- Con: computationally expensive, might stuck in local minima\n",
    "- Pro: promise to find optimal solution, low bias and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGD:\n",
    "\n",
    "    def __init__(self, max_iter = 1000, lr = 0.001, tol = 1e-6):\n",
    "        self.max_iter = max_iter\n",
    "        self.lr = lr\n",
    "        self.tol = tol\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "        self.bias = 0\n",
    "        loss = []\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            mse = (1/m) * np.sum(np.square(y_pred - y))\n",
    "            loss.append(mse)\n",
    "\n",
    "            dw = (1/m) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1/m) * np.sum(y_pred - y)\n",
    "\n",
    "            temp = self.weights.copy()\n",
    "            self.weights -= self.lr*dw\n",
    "            self.bias -= self.lr*db\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}, Loss: {mse}\")\n",
    "            \n",
    "            if all(abs(self.weights - temp) <= self.tol):\n",
    "                print(f\"Convergence reached at iteration {i}\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.dot(X, self.weights) + self.bias\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8852169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 4695.039202469317\n",
      "Iteration 100, Loss: 624.9014751951335\n",
      "Iteration 200, Loss: 122.3229556633559\n",
      "Iteration 300, Loss: 60.209367593174086\n",
      "Iteration 400, Loss: 52.5246998730196\n",
      "Iteration 500, Loss: 51.572785218556675\n",
      "Iteration 600, Loss: 51.454699857083725\n",
      "Iteration 700, Loss: 51.44002671206418\n",
      "Iteration 800, Loss: 51.4381998832959\n",
      "Iteration 900, Loss: 51.437971925882906\n",
      "Iteration 1000, Loss: 51.43794340647068\n",
      "Iteration 1100, Loss: 51.437939827779914\n",
      "Iteration 1200, Loss: 51.43793937718558\n",
      "Convergence reached at iteration 1287\n",
      "Weights (Coefficients): [54.59449546 38.50982028]\n",
      "Bias (Intercept): 0.07979662940059772\n",
      "R-Square (Coefficient of determination): 0.9852928347328443\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation ( Batch GD )\n",
    "\n",
    "lr = BatchGD(max_iter = 10000, lr = 0.01)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Weights (Coefficients): {lr.weights}\\nBias (Intercept): {lr.bias}\")\n",
    "\n",
    "y_test_pred = lr.predict(X_test)\n",
    "r2_score = metrics.r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"R-Square (Coefficient of determination): {score}\".format(score = r2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b52828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (Coefficients): [54.59459269 38.50985045]\n",
      "Bias (Intercept): 0.07979681999767206\n",
      "R-Square (Coefficient of determination): 0.9852928016810484\n"
     ]
    }
   ],
   "source": [
    "# sklearn implementation\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Weights (Coefficients): {model.coef_}\\nBias (Intercept): {model.intercept_}\")\n",
    "\n",
    "r2_score = model.score(X_test, y_test)\n",
    "print(\"R-Square (Coefficient of determination): {score}\".format(score = r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b98cfe",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "- Only use one sample randomly from all training instances at a time\n",
    "- Pro: computationally efficient, less likely to get stuck in local minima (irregular updates might help bounce off)\n",
    "- Con: high bias and variance (likely to overfit), irregular updates --> gradually reducing learning rate\n",
    "- Note: need to shuffle the data first to prevent embeded patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f94de1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGD:\n",
    "\n",
    "    def __init__(self, max_iter = 1000, t0 = 5, t1 = 50, tol = 1e-7):\n",
    "        self.max_iter = max_iter\n",
    "        self.t0 = t0 # initial learning rate\n",
    "        self.t1 = t1 # how quicly the learning rate decreases\n",
    "        self.tol = tol\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.weights = np.random.rand(n)\n",
    "        self.bias = np.random.rand(1)[0]\n",
    "        losses, flag = [], False\n",
    "\n",
    "        for epoch in range(1, self.max_iter + 1):\n",
    "            # randomly shuffle the data\n",
    "            random_idx = np.random.permutation(m)\n",
    "            X, y = X[random_idx], y[random_idx]\n",
    "            \n",
    "            if flag:\n",
    "                break\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}\\tLoss: {losses[epoch-1]}\")\n",
    "            \n",
    "            for i in range(m):\n",
    "                # randomly select one sample\n",
    "                rand_idx = np.random.randint(0, m)\n",
    "                X_i = X[rand_idx: rand_idx + 1]\n",
    "                y_i = y[rand_idx: rand_idx + 1]\n",
    "\n",
    "                y_pred = np.dot(X_i, self.weights) + self.bias\n",
    "                loss = np.sum(np.square(y_pred - y_i))\n",
    "                losses.append(loss)\n",
    "\n",
    "                dw = np.dot(X_i.T, y_pred - y_i)\n",
    "                db = np.sum(y_pred - y_i)\n",
    "                temp = self.weights.copy()\n",
    "                lr = self.learning_rate(m * epoch + i)\n",
    "                self.weights -= lr * dw\n",
    "                self.bias -= lr * db\n",
    "\n",
    "                if all(abs(self.weights - temp) <= self.tol):\n",
    "                    print(f\"Convergence reached at epoch {epoch}, iteration {i}\")\n",
    "                    flag = True\n",
    "                    break\n",
    "        \n",
    "    def learning_rate(self, t):\n",
    "        return self.t0 / (t + self.t1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.dot(X, self.weights) + self.bias\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26c40867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\tLoss: 1970.081006942571\n",
      "Epoch: 20\tLoss: 3388.6035069600257\n",
      "Epoch: 30\tLoss: 1048.025169320888\n",
      "Epoch: 40\tLoss: 19987.961561243857\n",
      "Epoch: 50\tLoss: 10714.48098531692\n",
      "Weights (Coefficients): [54.64361311 38.51682826]\n",
      "Bias (Intercept): 0.0502002727646547\n",
      "R-Square (Coefficient of determination): 0.9852789665256142\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation ( Stochastic GD )\n",
    "\n",
    "lr = StochasticGD(max_iter = 50, tol = 1e-8)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Weights (Coefficients): {lr.weights}\\nBias (Intercept): {lr.bias}\")\n",
    "\n",
    "y_test_pred = lr.predict(X_test)\n",
    "r2_score = metrics.r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"R-Square (Coefficient of determination): {score}\".format(score = r2_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f424cbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (Coefficients): [54.54019941 38.47934055]\n",
      "Bias (Intercept): [0.0721226]\n",
      "R-Square (Coefficient of determination): 0.9852960135227344\n"
     ]
    }
   ],
   "source": [
    "# Implementation through Sklean API\n",
    "\n",
    "model = linear_model.SGDRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Weights (Coefficients): {model.coef_}\\nBias (Intercept): {model.intercept_}\")\n",
    "\n",
    "r2_score = model.score(X_test, y_test)\n",
    "\n",
    "print(\"R-Square (Coefficient of determination): {score}\".format(score = r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb134e92",
   "metadata": {},
   "source": [
    "# MiniBatch Gradient Descent\n",
    "- choose random subset of the entire training set for gradient computation (e.g. 32 samples)\n",
    "- Pro: provides a performance boost with hardware optimization, with GPU\n",
    "- Con: may be stuck in local minima\n",
    "- Note: shuffle taining data to get rid of high bias/variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8757f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchGD:\n",
    "    def __init__(self, max_iter = 100, lr = 0.001, tol = 1e-6, batch_size = 32):\n",
    "        self.max_iter = max_iter\n",
    "        self.lr = lr\n",
    "        self.tol = tol\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.weights = np.random.rand(n)\n",
    "        self.bias = np.random.rand(1)[0]\n",
    "        costs, flag = [], False\n",
    "\n",
    "        for epoch in range(1, self.max_iter + 1):\n",
    "            perm_idx = np.random.permutation(m)\n",
    "            X, y = X[perm_idx], y[perm_idx]\n",
    "\n",
    "            if flag:\n",
    "                break\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch: {epoch}\\tLoss: {costs[epoch-1]}\")\n",
    "\n",
    "            for i in range(0, m, self.batch_size):\n",
    "                X_mini = X[i: i + self.batch_size]\n",
    "                y_mini = y[i: i + self.batch_size]\n",
    "\n",
    "                y_pred = np.dot(X_mini, self.weights) + self.bias\n",
    "                cost = 1 / self.batch_size * np.sum(np.square(y_pred - y_mini))\n",
    "                costs.append(cost)\n",
    "\n",
    "                dw = 1 / self.batch_size * np.dot(X_mini.T, (y_pred - y_mini))\n",
    "                db = 1 / self.batch_size * np.sum(y_pred - y_mini)\n",
    "                temp = self.weights.copy()\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "                if all(abs(self.weights - temp) <= self.tol):\n",
    "                    print(f\"Convergence reached at epoch {epoch}, iteration {i}\")\n",
    "                    flag = True\n",
    "                    break\n",
    "            \n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.dot(X, self.weights) + self.bias\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0881e70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100\tLoss: 3099.767009122487\n",
      "Epoch: 200\tLoss: 2765.425712432953\n",
      "Epoch: 300\tLoss: 2552.30274655026\n",
      "Epoch: 400\tLoss: 3035.4496211215064\n",
      "Epoch: 500\tLoss: 1566.4481698550503\n",
      "Epoch: 600\tLoss: 1061.3291798904215\n",
      "Epoch: 700\tLoss: 576.9637305208013\n",
      "Epoch: 800\tLoss: 927.9963193487199\n",
      "Epoch: 900\tLoss: 451.38913783901785\n",
      "Epoch: 1000\tLoss: 495.3531838985797\n",
      "Weights (Coefficients): [54.59459502 38.51002504]\n",
      "Bias (Intercept): 0.07999148008373759\n",
      "R-Square (Coefficient of determination): 0.9852929366749039\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation ( MiniBatch GD )\n",
    "\n",
    "lr = MiniBatchGD(max_iter = 1000, tol = 1e-7)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Weights (Coefficients): {lr.weights}\\nBias (Intercept): {lr.bias}\")\n",
    "\n",
    "y_test_pred = lr.predict(X_test)\n",
    "r2_score = metrics.r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"R-Square (Coefficient of determination): {score}\".format(score = r2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cb54937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (Coefficients): [48.63287984 34.75164334]\n",
      "Bias (Intercept): [0.31491738]\n",
      "R-Square (Coefficient of determination): 0.9737659442623807\n"
     ]
    }
   ],
   "source": [
    "# Implementation through Sklean API\n",
    "\n",
    "def form_batches(X, y, batch_size = 32):\n",
    "    m, n = X.shape\n",
    "    X_batches, y_batches = [], []\n",
    "    \n",
    "    for i in range(0, m, batch_size):\n",
    "        perm_idx = np.random.permutation(m)\n",
    "        X, y = X[perm_idx], y[perm_idx]\n",
    "        \n",
    "        X_batch = X[i: i + batch_size]\n",
    "        y_batch = y[i: i + batch_size]\n",
    "\n",
    "        if X_batch.shape[0] < batch_size:\n",
    "            continue\n",
    "        \n",
    "        X_batches.append(X_batch)\n",
    "        y_batches.append(y_batch)\n",
    "        \n",
    "    return np.array(X_batches), np.array(y_batches)\n",
    "\n",
    "model = linear_model.SGDRegressor()\n",
    "X_batches, y_batches = form_batches(X_train, y_train)\n",
    "\n",
    "for X, y in zip(X_batches, y_batches):\n",
    "    model.partial_fit(X, y)\n",
    "\n",
    "print(f\"Weights (Coefficients): {model.coef_}\\nBias (Intercept): {model.intercept_}\")\n",
    "\n",
    "r2_score = model.score(X_test, y_test)\n",
    "print(\"R-Square (Coefficient of determination): {score}\".format(score = r2_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
